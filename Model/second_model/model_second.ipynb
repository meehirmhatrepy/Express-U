{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dropout,Dense,ReLU,Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'D:\\Express-U\\data\\live_dataset\\train'\n",
    "test_path = r'D:\\Express-U\\data\\live_dataset\\test' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24535 images belonging to 35 classes.\n",
      "Found 1435 images belonging to 35 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function= tensorflow.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(35,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 35)                4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 417,571\n",
      "Trainable params: 417,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2454/2454 [==============================] - 611s 249ms/step - loss: 0.2459 - accuracy: 0.9432 - val_loss: 0.0030 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "2454/2454 [==============================] - 104s 42ms/step - loss: 5.4536e-04 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "2454/2454 [==============================] - 105s 43ms/step - loss: 3.9663e-04 - accuracy: 1.0000 - val_loss: 6.1660e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 4/10\n",
      "2454/2454 [==============================] - 106s 43ms/step - loss: 2.2493e-04 - accuracy: 1.0000 - val_loss: 5.0453e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "2454/2454 [==============================] - 109s 45ms/step - loss: 1.8643e-04 - accuracy: 1.0000 - val_loss: 4.2744e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "2454/2454 [==============================] - 111s 45ms/step - loss: 1.5881e-04 - accuracy: 1.0000 - val_loss: 3.7062e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "2454/2454 [==============================] - 114s 47ms/step - loss: 1.3939e-04 - accuracy: 1.0000 - val_loss: 3.3001e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "2454/2454 [==============================] - 113s 46ms/step - loss: 1.2352e-04 - accuracy: 1.0000 - val_loss: 2.9990e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "2454/2454 [==============================] - 107s 44ms/step - loss: 1.1091e-04 - accuracy: 1.0000 - val_loss: 2.7320e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "2454/2454 [==============================] - 103s 42ms/step - loss: 1.0049e-04 - accuracy: 1.0000 - val_loss: 2.4754e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0002751646679826081, 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "model.evaluate(imgs, labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_second.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2454/2454 [==============================] - 608s 247ms/step - loss: 0.3677 - accuracy: 0.9066 - val_loss: 0.0258 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "2454/2454 [==============================] - 116s 47ms/step - loss: 0.0511 - accuracy: 0.9888 - val_loss: 0.0048 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "2454/2454 [==============================] - 109s 45ms/step - loss: 0.0729 - accuracy: 0.9885 - val_loss: 3.3793e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "2454/2454 [==============================] - 93s 38ms/step - loss: 0.0604 - accuracy: 0.9931 - val_loss: 0.0136 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "2454/2454 [==============================] - 116s 47ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.6145e-05 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
      "Epoch 6/10\n",
      "2454/2454 [==============================] - 112s 46ms/step - loss: 1.0460e-05 - accuracy: 1.0000 - val_loss: 6.1086e-06 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "2454/2454 [==============================] - 128s 52ms/step - loss: 4.9585e-06 - accuracy: 1.0000 - val_loss: 3.3413e-06 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "2454/2454 [==============================] - 130s 53ms/step - loss: 2.3310e-06 - accuracy: 1.0000 - val_loss: 1.2352e-06 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "2454/2454 [==============================] - 133s 54ms/step - loss: 8.3123e-07 - accuracy: 1.0000 - val_loss: 4.3097e-07 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "2454/2454 [==============================] - 113s 46ms/step - loss: 2.5831e-07 - accuracy: 1.0000 - val_loss: 1.2328e-07 - val_accuracy: 1.0000 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches) \n",
    "model.evaluate(imgs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_second_adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b5871bf28a57d4b9157c5a5a9f61e3b60ec91a012fb12b356be1a8edc8bee2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
